---
title: "R Notebook"
output: html_notebook
---
## Problem 2: Sums of Squares

a. Using for loop
```{r}
# Generate the data
set.seed(12345)
y <- seq(from = 1, to = 100, length.out = 1e+08) + rnorm(1e+08)

# Create for loop to calculate Syy
loop_time <- system.time({
  Syy <- 0
  mean_y <- mean(y)

  for (i in 1:length(y)) {
    Syy <- Syy + (y[i]-mean_y)^2    
  }
})
```

b. Use vector operations

```{r}
vect_time <- system.time({
  diffSq <- (y-mean_y)^2
  Syy <- sum(diffSq)
})

# report the result
report1 <- data.frame(Method = c("Loop","Vector Ops"), User =c(loop_time[1],vect_time[1]), System =c(loop_time[2],vect_time[2]), Elapsed =c(loop_time[3],vect_time[3]))

knitr::kable(report1,caption="Running time comparison table")
```

Using vector operations is a lot faster than using a for loop.

## Problem 3: Using the dual nature to our advantage

```{r}
set.seed(1256)
theta <- as.matrix(c(1, 2), nrow = 2)
X <- cbind(1, rep(1:10, 10))
h <- X %*% theta + rnorm(100, 0, 0.2)

tolerance <- as.matrix(c(1e-5,1e-5),nrow=2)
alpha <- 0.01 #Step size
m <- nrow(h)
  
# old_theta hold values of theta from last iteration
old_theta <- as.matrix(c(10,10),nrow=2) # an arbitray value to start the loop

# new_theta hold values of theta calculating from the data and old theta 
new_theta <- as.matrix(c(8,8),nrow=2)

counter <- 0
while(sum(abs(new_theta - old_theta) > tolerance) == 2) {
  old_theta <- new_theta
  
  h0x <- old_theta[1,] + old_theta[2,]*X[,2]
  
  new_theta[1,] <- old_theta[1,] - alpha/m*sum(h0x-h[,1])
  
  new_theta[2,] <- old_theta[2,] - alpha/m*sum((h0x-h[,1])*X[,2])
  
  counter <- counter + 1

}

#Get least square estimations of coefficent
lmfit <- lm(h~0+X)

#Build a comparison table
result3 <- data.frame(Method = c("Our Method","LSE"), Beta_0 = c(new_theta[1,],lmfit$coefficients[1]),Beta_1 = c(new_theta[2,],lmfit$coefficients[2]))

knitr::kable(result3,caption="Coefficient Estimates Comparison Between 2 methods")

```

## Problem 4: Inverting matrices

Finding $\hat{\beta}$ by computing $(X'X)^{-1}X'\underline{y}$ will be a computationally expensive operation since it involves a matrix multiplication followed by an invert and another 2 matrix multiplications. If $n$, the number of observations, is a very large number, chances are we will use up a lot of memory to find just a 2x1 matrix of $\hat{\beta}$.

In this case, I will rely on the numerical solution for $\hat{\beta}$ and calculate $\hat{\beta_0}$ and $\hat{\beta_1}$ separately using vector operations.

## Problem 5: Need for speed

```{r}
    set.seed(12456) 
    
    G <- matrix(sample(c(0,0.5,1),size=16000,replace=T),ncol=10)
    R <- cor(G) # R: 10 * 10 correlation matrix of G
    C <- kronecker(R, diag(1600)) # C is a 16000 * 16000 block diagonal matrix
    id <- sample(1:16000,size=932,replace=F)
    q <- sample(c(0,0.5,1),size=15068,replace=T) # vector of length 15068
    A <- C[id, -id] # matrix of dimension 932 * 15068
    B <- C[-id, -id] # matrix of dimension 15068 * 15068
    p <- runif(932,0,1)
    r <- runif(15068,0,1)
    C<-NULL #save some memory space
```

a. Size of A and B
```{r}
# Size of object A:
object.size(A)

# Size of object B:
object.size(B)
```

Calculating $y = p + AB^{-1}(q-r)$ consists of several vector and matrix addition, one matrix inversion, and two matrix multiplications. The first operation is vector subtraction $q-r$. Complexity for this operation is linear, which means equal the size of the vector, 15068. Next, matrix inversion takes up complexity of $O(n^3)$, which in this case is $15068^3$. Then, matrix multiplcation between $A_{n\times m}$ and $B^{-1}_{m \times p}$ takes up $O(mnp)$. Specifically, it will take $932 \times 15068^2$ in our case. Next, matrix multiplication between $AB^{-1}$ and $(q-r)$ takes up $932\times15068$. Finally, vector addition between $p$ and $AB^{-1}(q-r)$ takes up linear time, which is $932$ operations. 

All in all, it will take $15068 + 15068^3 + 932\times15068^2+932\times15068 + 932$ operations to compute $y$.

b. Optimization

Recognizing that matrix B already consists of a diagonal of $1$'s and because of the nature of subsetting from a kronecker matrix coming from a matrix and identity matrix, most of the values in B will consist of 0's. Thus, instead of using the "solve" function, we can program our own function to invert matrices by using a parralel Identity matrix as inspired by the Gauss-Jordan elimination. By using vector addition and scalar multiplication, we hope to reduce the run time instead of going through the "solve" function.

```{r}

# Using my own matrix multiplication functions in R based on the properties of A and B
op.time.1 <- system.time({
  
  # Import inverse function from the RcppArmadillo package since things run faster in Cpp
  library(RcppArmadillo)
  library(Rcpp)
  sourceCpp("arma_functions.cpp")

#Multiplying A and B_invert
# Since each row i of A and columns of B contain a lot of zeros,
# non-zero dot product only occur when there exist positions within # the row and column where the values are non-zero

## Initialize an empty result matrix

mat.mat.mult <- function (A,B) {
  AB <- matrix(0L,nrow=nrow(A),ncol=ncol(B))

  #Create a list of rows of A that contain nonzero elements
  nonZeroRow <- c()
  for (i in 1:nrow(A)){
    if (any(A[i,] != 0)){
      nonZeroRow <- c(nonZeroRow,i)
    }
  }
  
  nonZeroCol <- c()
  #Create a list of columns of A that contain nonzero elements
  for (j in 1:ncol(B)){
    if (any(B[,j] != 0)){
      nonZeroCol <- c(nonZeroCol,j)
    }
  }
  
  #Loop through each element to calculate the dot product
  
  for (Ar in nonZeroRow)
  {
    # Obtain the position in A row where nonzero element exists
    notZero_A <- which(A[Ar,] != 0)
    for (Bc in nonZeroCol) {
      # Obtain the position in B column where nonzero element exists
      notZero_B <- which(B[,Bc] != 0)
      
      #Find the common position of nonzero between A's row i and B's col j
      both_nonZero <- intersect(notZero_A,notZero_B)
    
    #If exists, then perform dot product and assign to A.invB[i,j]
      if (length(both_nonZero) > 0) {
        AB[Ar,Bc] <- A[Ar,both_nonZero] %*% B[both_nonZero,Bc]
      }
      
    }
    
  }

 return (AB) 
}

# Calculate A*inv(B)*(q-r)

mat.vect.mult <- function(M,v) {
  result <- rep(0,nrow(M))

for (i in 1:nrow(M)) {
  #Apply the same scheme, only rows containing non-zero values matter
  if (any(M[i,] != 0)){
    nonZero_posit <- which(M[i,] != 0)
    result[i] <- M[i,nonZero_posit] %*% v[nonZero_posit]
  }
    
  } 
  return(result)
}
my_Y_1 <- p + mat.vect.mult(mat.mat.mult(A,getInverse(B)),q-r)
})

# No loading time
op.time.1.noLoading <- system.time({
  my_Y_1 <- p + mat.vect.mult(mat.mat.mult(A,solve(B)),q-r)
})

```
c. Put everything to test

```{r}
# Non-optimization way
non.op.time <- system.time(
  {
    true_Y <- p + A%*%solve(B)%*%(q-r)
  }
)

#Check result
plot(true_Y,my_Y_1,pch=16,main="True values vs results from optimization")

# Comparison
report5 <- data.frame(Method = c("Non optimization","Optimization 1","Opt.1 without loading"),User =c(non.op.time[1],op.time.1[1],op.time.1.noLoading[1]), System =c(non.op.time[2],op.time.1[2],op.time.1.noLoading[2]), Elapsed =c(non.op.time[3],op.time.1[3],op.time.1.noLoading[3]))
 
knitr::kable(report5,caption="Running time comparison table")

```

The comparison table shows that with optimization, the time it takes to compute y is still bigger than that of using standard R operations. 


